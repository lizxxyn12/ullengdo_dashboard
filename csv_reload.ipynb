{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "police_path = \"울릉경찰서 교통계(교통사고)_2025년도.csv\"\n",
    "ulleung_path = \"ulleung_accidents_with_coords.csv\"\n",
    "\n",
    "police = pd.read_csv(police_path)\n",
    "ulleung = pd.read_csv(ulleung_path)\n",
    "\n",
    "place_col = [c for c in police.columns if \"사고\" in c and \"장소\" in c][0]\n",
    "type_col = [c for c in police.columns if \"종별\" in c][0]\n",
    "\n",
    "\n",
    "def normalize(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).replace(\"\n",
    "\", \" \")\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "police = police[[place_col, type_col]].copy()\n",
    "police[place_col] = police[place_col].map(normalize)\n",
    "police[type_col] = police[type_col].map(normalize)\n",
    "police = police[(police[place_col] != \"\") & (police[type_col] != \"\")]\n",
    "\n",
    "police = police.drop_duplicates(subset=[place_col])\n",
    "\n",
    "mapping = dict(zip(police[place_col], police[type_col]))\n",
    "\n",
    "ulleung[\"type\"] = ulleung[\"raw\"].map(lambda s: mapping.get(normalize(s), pd.NA))\n",
    "ulleung.to_csv(ulleung_path, index=False)\n",
    "\n",
    "missing = ulleung[ulleung[\"type\"].isna()][[\"raw\"]]\n",
    "print(\"missing count\", len(missing))\n",
    "print(missing.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ulleung_accidents_with_coords.csv\n",
      "  Done. Saved to ulleung_accidents_with_coords.csv\n",
      "Processing: 울릉경찰서 교통계(교통사고)_2019년도_with_coords.csv\n",
      "  Done. Saved to 울릉경찰서 교통계(교통사고)_2019년도_with_coords.csv\n",
      "Processing: 울릉경찰서 교통계(교통사고)_2020년도_with_coords.csv\n",
      "  Done. Saved to 울릉경찰서 교통계(교통사고)_2020년도_with_coords.csv\n",
      "Processing: 울릉경찰서 교통계(교통사고)_2021년도_with_coords.csv\n",
      "  Done. Saved to 울릉경찰서 교통계(교통사고)_2021년도_with_coords.csv\n",
      "Processing: 울릉경찰서 교통계(교통사고)_2022년도_with_coords.csv\n",
      "  Done. Saved to 울릉경찰서 교통계(교통사고)_2022년도_with_coords.csv\n",
      "Processing: 울릉경찰서 교통계(교통사고)_2023년도_with_coords.csv\n",
      "  Done. Saved to 울릉경찰서 교통계(교통사고)_2023년도_with_coords.csv\n",
      "Processing: 울릉경찰서 교통계(교통사고)_2024년도_with_coords.csv\n",
      "  Done. Saved to 울릉경찰서 교통계(교통사고)_2024년도_with_coords.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "\n",
    "# 파일 찾기\n",
    "def find_target_files(root: Path):\n",
    "    # _with_coords.csv 로 끝나는 파일들을 찾습니다.\n",
    "    return sorted(list(root.glob(\"*_with_coords.csv\")))\n",
    "\n",
    "def normalize_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    # 유니코드 정규화 및 줄바꿈/다중공백 제거\n",
    "    text = unicodedata.normalize(\"NFC\", str(text))\n",
    "    text = text.replace(\"\\n\", \" \").strip()\n",
    "    return re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "def parse_address(full_addr):\n",
    "    \"\"\"\n",
    "    주소 문자열을 분석하여 (clean_addr, detail) 튜플을 반환합니다.\n",
    "    예: \"경북 울릉군 ...로 550 경북 울릉군 ...로 550 식당앞\" \n",
    "    -> (\"경상북도 울릉군 ...로 550\", \"식당앞\")\n",
    "    \"\"\"\n",
    "    full_addr = normalize_text(full_addr)\n",
    "    if not full_addr:\n",
    "        return \"\", \"\"\n",
    "\n",
    "    # 1. 정규식 정의\n",
    "    # 도로명 주소 패턴: 시/도 + 시/군/구 + 로/길 + 번호\n",
    "    # (공백이 불규칙할 수 있으므로 \\s* 사용, 번호는 123-4 형식 포함)\n",
    "    pat_road = r'((?:경상북도|경북)\\s+(?:울릉군|울릉)\\s+[가-힣0-9]+(?:로|길)\\s+\\d+(?:\\s*-\\s*\\d+)?)'\n",
    "    \n",
    "    # 지번 주소 패턴: 시/도 + 시/군/구 + 읍/면 + (리) + 번지\n",
    "    # 예: 경북 울릉 북면 현포 296- 2번지\n",
    "    pat_jibun = r'((?:경상북도|경북)\\s+(?:울릉군|울릉)\\s+[가-힣]+(?:읍|면|동)(?:\\s+[가-힣]+리)?\\s+\\d+(?:\\s*-\\s*\\d+)?(?:번지)?)'\n",
    "\n",
    "    # 매칭 시도 (도로명 우선)\n",
    "    match = re.search(pat_road, full_addr)\n",
    "    if not match:\n",
    "        match = re.search(pat_jibun, full_addr)\n",
    "    \n",
    "    if match:\n",
    "        clean_addr = match.group(1)\n",
    "        # 정규화: '경북' -> '경상북도', '울릉' -> '울릉군' 등 표준화가 필요하면 여기서 처리\n",
    "        # 일단은 추출된 문자열을 공백 정리하여 사용\n",
    "        clean_addr = re.sub(r'\\s+', ' ', clean_addr).strip()\n",
    "        \n",
    "        # detail 추출 로직\n",
    "        # 1. 원본 문자열에서 clean_addr 부분을 제거해본다.\n",
    "        # 단순히 replace를 쓰면 중간에 있는 것도 지워질 수 있으므로, \n",
    "        # \"주소가 문자열의 시작 부분에 반복해서 나오는지\" 확인하는 것이 좋습니다.\n",
    "        \n",
    "        # 정규식 매치된 부분의 끝 인덱스\n",
    "        end_pos = match.end()\n",
    "        remainder = full_addr[end_pos:].strip()\n",
    "        \n",
    "        # 만약 남은 문자열이 또다시 주소로 시작한다면? (중복 기재된 경우)\n",
    "        # 예: \"주소 주소 상세\" -> \" 상세\"\n",
    "        # 정확한 문자열 매칭보다는, 정규식으로 다시 확인\n",
    "        match_again = re.match(pat_road, remainder) or re.match(pat_jibun, remainder)\n",
    "        if match_again:\n",
    "            # 두 번째 주소 뒤에 있는 부분이 진짜 detail\n",
    "            detail = remainder[match_again.end():].strip()\n",
    "        else:\n",
    "            # 주소가 한 번만 나온 경우, 그 뒤가 detail\n",
    "            detail = remainder\n",
    "            \n",
    "        return clean_addr, detail\n",
    "    else:\n",
    "        # 주소 패턴을 못 찾은 경우\n",
    "        return \"\", full_addr\n",
    "\n",
    "def process_file(path: Path):\n",
    "    print(f\"Processing: {path.name}\")\n",
    "    \n",
    "    # CSV 읽기\n",
    "    try:\n",
    "        df = pd.read_csv(path, encoding='utf-8-sig')\n",
    "    except:\n",
    "        df = pd.read_csv(path, encoding='cp949') # fallback\n",
    "\n",
    "    # 컬럼 이름 공백 정리\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    # 원본 주소 컬럼 찾기 (raw가 있으면 raw, 없으면 사고장소)\n",
    "    src_col = 'raw' if 'raw' in df.columns else '사고장소'\n",
    "    if src_col not in df.columns:\n",
    "        # 둘 다 없으면 가장 긴 문자열을 가진 컬럼을 주소로 추정하거나 스킵\n",
    "        print(f\"  Skipping: source address column not found.\")\n",
    "        return\n",
    "\n",
    "    # 새로운 데이터 리스트 생성\n",
    "    clean_list = []\n",
    "    detail_list = []\n",
    "    \n",
    "    for val in df[src_col]:\n",
    "        c, d = parse_address(val)\n",
    "        clean_list.append(c)\n",
    "        detail_list.append(d)\n",
    "\n",
    "    # 결과 데이터프레임 생성 (순서 지정)\n",
    "    # 기존 lat/lon, type이 있다면 유지\n",
    "    out_df = pd.DataFrame()\n",
    "    out_df['raw'] = df[src_col].apply(normalize_text) # 원본\n",
    "    out_df['detail'] = detail_list\n",
    "    out_df['clean_normalized'] = clean_list\n",
    "    \n",
    "    if 'latitude' in df.columns:\n",
    "        out_df['latitude'] = df['latitude']\n",
    "    if 'longitude' in df.columns:\n",
    "        out_df['longitude'] = df['longitude']\n",
    "        \n",
    "    # type 컬럼 찾기 (사고종별, type 등)\n",
    "    type_col = next((c for c in df.columns if '종별' in c or 'type' in c.lower()), None)\n",
    "    if type_col:\n",
    "        out_df['type'] = df[type_col]\n",
    "\n",
    "    # 저장\n",
    "    out_df.to_csv(path, index=False, encoding='utf-8-sig')\n",
    "    print(f\"  Done. Saved to {path.name}\")\n",
    "\n",
    "# 실행\n",
    "files = find_target_files(Path(\".\"))\n",
    "if not files:\n",
    "    print(\"No target files found (*_with_coords.csv).\")\n",
    "else:\n",
    "    for f in files:\n",
    "        process_file(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['KT', '감염병예방의약팀', '감영병예방의약팀', '건강출산팀', '건설과', '건영택배', '경제교통정책실', '경제투자유치실', '공보팀', '공항건설행정팀', '관광개발팀', '관광기획팀', '관광문화체육과', '관광문화체육실', '관광산림과', '관제센터', '교육인구정책팀', '교통정책과', '교통지도팀', '교통항공팀', '교통행정팀', '국민건강보험공단', '금광해운', '기술보급과', '기술보급팀', '기획감사실', '기획법무팀', '긴급재난지원금지원팀', '농업기술센터', '농업산림과', '농업유통과', '농정팀', '대저건설', '대저페리', '대저해운', '대저해운 도착시간', '도로토목팀', '도시건축과', '독도관리사무소', '독도박물관', '독도크루즈', '롯데택배', '문화예술팀', '문화체육과', '미래전략추진단', '미래전략팀', '미래해운', '민원팀', '방재하천팀', '보건사업과', '보건의료원', '북면사무소', '산림팀', '상하수도사업소', '서면사무소', '세정팀', '수산정책팀', '시설관리사업소', '썬라이즈 도착시간', '씨스포빌', '씨스포빌 도착시간', '안전건설과', '안전건설단', '안전도시과', '안전정책팀', '언론홍보팀', '에이치해운', '여객항만팀', '여성가족팀', '우성해운', '울릉경비대', '울릉경찰서', '울릉경찰서 교통계', '울릉경찰서 범죄예방계', '울릉경찰서 생활안전교통과', '울릉경찰서 수사과', '울릉교육지원청', '울릉군 민방위상황실', '울릉군 재난안전대책본부', '울릉군가족센터', '울릉군긴급재난지원팀', '울릉군여성단체협의회', '울릉군청', '울릉도관측소', '울릉도독도해양연구기지', '울릉문화원', '울릉우체국', '울릉읍사무소', '울릉크루즈', '원무팀', '위생팀', '의회사무과', '인구정책팀', '인재육성팀', '일자리경제교통과', '일자리경제팀', '자치행정과', '재무과', '정보통신팀', '제이에이치페리', '제이에이치페리 도착시간', '주민복지과', '주식회사태성해운', '주택건축팀', '지역개발과', '지역건설팀', '지역계획팀', '지적팀', '체육지원팀', '총무과', '축산방역팀', '친환경농업팀', '친환경에너지팀', '태성해운', '태성해운 도착시간', '통합방위종합상황실', '투자유치팀', '한국수자원공사 울릉현대화사업소', '한국전력공사', '한진, CJ대한통운', '한진택배', '해군 제1함대사령부 118전대', '해군제118전대', '해군제118조기경보전대', '해양수산과', '행정담당', '행정팀', '환경미화팀', '환경위생과', '환경지질팀']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "sms_path = \"울릉알리미_텍스트.csv\"\n",
    "\n",
    "names = set()\n",
    "with open(sms_path, newline=\"\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        name = (row.get(\"sms_mem_name\") or \"\").strip()\n",
    "        if name:\n",
    "            names.add(name)\n",
    "\n",
    "print(sorted(names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved sms_msg_classified.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "sms_path = \"울릉알리미_텍스트.csv\"\n",
    "out_path = \"sms_msg_classified.csv\"\n",
    "\n",
    "ship_keywords = [\n",
    "    \"금광해운\",\n",
    "    \"대저해운\",\n",
    "    \"대저해운 도착시간\",\n",
    "    \"에이치해운\",\n",
    "    \"우성해운\",\n",
    "    \"주식회사태성해운\",\n",
    "    \"태성해운 도착시간\",\n",
    "]\n",
    "people_keywords = [\n",
    "    \"대저페리\",\n",
    "    \"썬라이즈 도착시간\",\n",
    "    \"씨스포빌\",\n",
    "    \"씨스포빌 도착시간\",\n",
    "    \"울릉크루즈\",\n",
    "    \"제이에이치페리\",\n",
    "    \"제이에이치페리 도착시간\",\n",
    "]\n",
    "cancel_keywords = [\n",
    "    \"결항\",\n",
    "    \"운항 통제\",\n",
    "    \"운항통제\",\n",
    "    \"운항이 통제\",\n",
    "    \"운항없음\",\n",
    "    \"운항 없음\",\n",
    "    \"운항 취소\",\n",
    "    \"출항 취소\",\n",
    "    \"출항불가\",\n",
    "    \"휴항\",\n",
    "    \"운항 중지\",\n",
    "    \"취소되었습니다\",\n",
    "    \"통제되었습니다\",\n",
    "]\n",
    "depart_keywords = [\n",
    "    \"출항\",\n",
    "    \"출발\",\n",
    "    \"운항합니다\",\n",
    "    \"정상운항\",\n",
    "    \"운항 예정\",\n",
    "]\n",
    "arrive_keywords = [\n",
    "    \"입항 예정\",\n",
    "    \"입항 예정시간\",\n",
    "    \"입항입니다\",\n",
    "    \"입항 예정 시간\",\n",
    "]\n",
    "exclude_keywords = [\n",
    "    \"셔틀\",\n",
    "]\n",
    "\n",
    "def classify_category(msg: str) -> str:\n",
    "    if not msg:\n",
    "        return \"미분류\"\n",
    "    for k in ship_keywords:\n",
    "        if k in msg:\n",
    "            return \"선박\"\n",
    "    for k in people_keywords:\n",
    "        if k in msg:\n",
    "            return \"사람\"\n",
    "    return \"미분류\"\n",
    "\n",
    "def classify_status(msg: str) -> str:\n",
    "    if not msg:\n",
    "        return \"미정\"\n",
    "    for k in cancel_keywords:\n",
    "        if k in msg:\n",
    "            return \"운항불가\"\n",
    "    for k in arrive_keywords:\n",
    "        if k in msg:\n",
    "            return \"입항 예정\"\n",
    "    for k in depart_keywords:\n",
    "        if k in msg:\n",
    "            return \"출항\"\n",
    "    return \"미정\"\n",
    "\n",
    "with open(sms_path, newline=\"\", encoding=\"utf-8\") as f, open(\n",
    "    out_path, \"w\", newline=\"\", encoding=\"utf-8\"\n",
    ") as out:\n",
    "    reader = csv.DictReader(f)\n",
    "    fieldnames = [\"sms_msg\", \"sms_resDate\", \"category\", \"status\"]\n",
    "    writer = csv.DictWriter(out, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for row in reader:\n",
    "        msg = (row.get(\"sms_msg\") or \"\").strip()\n",
    "        if any(k in msg for k in exclude_keywords):\n",
    "            continue\n",
    "        category = classify_category(msg)\n",
    "        if category == \"미분류\":\n",
    "            continue\n",
    "        status = classify_status(msg)\n",
    "        writer.writerow({\n",
    "            \"sms_msg\": msg,\n",
    "            \"sms_resDate\": (row.get(\"sms_resDate\") or \"\").strip(),\n",
    "            \"category\": category,\n",
    "            \"status\": status,\n",
    "        })\n",
    "\n",
    "print(\"saved\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025 status counts\n",
      "입항 예정: 295\n",
      "미정: 13\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "classified_path = \"sms_msg_classified.csv\"\n",
    "\n",
    "counts = Counter()\n",
    "with open(classified_path, newline=\"\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        res_date = (row.get(\"sms_resDate\") or \"\").strip()\n",
    "        if not res_date:\n",
    "            continue\n",
    "        res_date = res_date.replace(\".\", \"-\").replace(\"/\", \"-\")\n",
    "        try:\n",
    "            dt = datetime.strptime(res_date, \"%Y-%m-%d %H:%M\")\n",
    "        except ValueError:\n",
    "            try:\n",
    "                dt = datetime.strptime(res_date, \"%Y-%m-%d\")\n",
    "            except ValueError:\n",
    "                continue\n",
    "        if dt.year != 2025:\n",
    "            continue\n",
    "        status = (row.get(\"status\") or \"미정\").strip()\n",
    "        counts[status] += 1\n",
    "\n",
    "print(\"2025 status counts\")\n",
    "for k, v in counts.most_common():\n",
    "    print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "금일(01일) 울릉크루즈 울릉(사동항) 입항 예정시간은 08:00분이며, 탑승인원은 1,200명 입니다\n",
      "금일(02일) 울릉크루즈 울릉(사동항) 입항 예정시간은 07:20분이며, 탑승인원은 383명 입니다\n",
      "금일(03일) 울릉크루즈 울릉(사동항) 입항 예정시간은 07:20분이며, 탑승인원은 237명 입니다\n",
      "금일(04일) 울릉크루즈 울릉(사동항) 입항 예정시간은 07:20분이며, 탑승인원은 456명 입니다\n",
      "금일(05일) 울릉크루즈 울릉(사동항) 입항 예정시간은 07:20분이며, 탑승인원은 202명 입니다\n",
      "금일(06일) 울릉크루즈 울릉(사동항) 입항 예정시간은 07:20분이며, 탑승인원은 391명 입니다\n",
      "금일(11일) 울릉크루즈 울릉(사동항) 입항 예정시간은 07:20분이며, 탑승인원은 603명 입니다\n",
      "금일(12일) 울릉크루즈 울릉(사동항) 입항 예정시간은 07:20분이며, 탑승인원은 311명 입니다\n",
      "금일(13일) 울릉크루즈 울릉(사동항) 입항 예정시간은 07:20분이며, 탑승인원은 246명 입니다\n",
      "금일(14일) 울릉크루즈 울릉(사동항) 입항 예정시간은 07:30분이며, 탑승인원은 418명 입니다\n",
      "printed 10\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "classified_path = \"sms_msg_classified.csv\"\n",
    "\n",
    "def parse_dt(value: str):\n",
    "    value = value.replace(\".\", \"-\").replace(\"/\", \"-\")\n",
    "    for fmt in (\"%Y-%m-%d %H:%M\", \"%Y-%m-%d\"):\n",
    "        try:\n",
    "            return datetime.strptime(value, fmt)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "printed = 0\n",
    "with open(classified_path, newline=\"\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        status = (row.get(\"status\") or \"\").strip().replace(\" \", \"\")\n",
    "        if status != \"입항예정\":\n",
    "            continue\n",
    "        res_date = (row.get(\"sms_resDate\") or \"\").strip()\n",
    "        dt = parse_dt(res_date)\n",
    "        if not dt or dt.year != 2025:\n",
    "            continue\n",
    "        print(row.get(\"sms_msg\"))\n",
    "        printed += 1\n",
    "        if printed >= 10:\n",
    "            break\n",
    "\n",
    "print(\"printed\", printed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입항예정 샘플\n",
      "금일(01일) 울릉크루즈 울릉(사동항) 입항 예정시간은 08:00분이며, 탑승인원은 1,200명 입니다\n",
      "금일(02일) 울릉크루즈 울릉(사동항) 입항 예정시간은 07:20분이며, 탑승인원은 383명 입니다\n",
      "금일(03일) 울릉크루즈 울릉(사동항) 입항 예정시간은 07:20분이며, 탑승인원은 237명 입니다\n",
      "금일(04일) 울릉크루즈 울릉(사동항) 입항 예정시간은 07:20분이며, 탑승인원은 456명 입니다\n",
      "금일(05일) 울릉크루즈 울릉(사동항) 입항 예정시간은 07:20분이며, 탑승인원은 202명 입니다\n",
      "금일(06일) 울릉크루즈 울릉(사동항) 입항 예정시간은 07:20분이며, 탑승인원은 391명 입니다\n",
      "금일(11일) 울릉크루즈 울릉(사동항) 입항 예정시간은 07:20분이며, 탑승인원은 603명 입니다\n",
      "금일(12일) 울릉크루즈 울릉(사동항) 입항 예정시간은 07:20분이며, 탑승인원은 311명 입니다\n",
      "금일(13일) 울릉크루즈 울릉(사동항) 입항 예정시간은 07:20분이며, 탑승인원은 246명 입니다\n",
      "금일(14일) 울릉크루즈 울릉(사동항) 입항 예정시간은 07:30분이며, 탑승인원은 418명 입니다\n",
      "printed 10\n",
      "\n",
      "미정 샘플\n",
      "엘도라도EX호 2025년 울릉주민 정기권 발행을 시작하였습니다. 자세한 사항은 대저페리 홈페이지를 확인하여 주시기 바랍니다.\n",
      "*정정*엘도라도EX호 2025년 울릉주민 정기권 3월부터 발행 예정입니다. 자세한 사항은 대저페리 홈페이지를 확인하여 주시기 바랍니다.\n",
      "내일(28일)부터 엘도라도EX호 2025년 울릉주민 정기권 발행을 시작합니다. 자세한 사항은 대저페리 홈페이지를 확인하여 주시기 바랍니다.\n",
      "[한진택배] 사동항 울릉크루즈 앞 오전7시부터 당일택배 접수받습니다. 4월1일 부터 5월말까지 주민분들의 많은 관심 부탁드립니다.\n",
      "? 울릉크루즈 당일 주민 승선권 발권 알림    - 기      간 : 2025. 5. 19.(월)부터    - 내      용 : 군민 당일 승선권 30매    - 발권방법 : 선착순   \"울릉군은 군민 승선권 확\n",
      "? 울릉크루즈 당일 주민 승선권 발권 알림   - 기      간 : 2025. 5. 19.(월)부터   - 내      용 : 군민 당일 승선권 30매   - 발권방법 : 선착순\n",
      "울릉크루즈 주민표 발권에 대하여 안내 말씀 드립니다.\n",
      "울릉크루즈는 울릉주민 판매분 20석+당일 현장 취소분 10석 등 총 30석을 확보할 예정이오며, 사전결제가 진행되었거나 이미 매진이 된 항차에 있어서는 좌석확보가 어려울 수도 있습니다.\n",
      "울릉크루즈 선박전검으로 인하여2025. 07. 02. ~ 07.03.(2일간) 현포마을 일일택배를 운영하지 않으니 이용에 불편없으시길 바랍니다.\n",
      "울릉크루즈 입항시 출구쪽 양쪽 도로에 불법 주.정차 차량으로 보행자의 안전 위험이 매우 높음으로 주민 및 울릉을 방문하는 관광객의 안전을 위하여 사동항 주차장을 이용 부탁드립니다\n",
      "printed 10\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "classified_path = \"sms_msg_classified.csv\"\n",
    "\n",
    "def parse_dt(value: str):\n",
    "    value = value.replace(\".\", \"-\").replace(\"/\", \"-\")\n",
    "    for fmt in (\"%Y-%m-%d %H:%M\", \"%Y-%m-%d\"):\n",
    "        try:\n",
    "            return datetime.strptime(value, fmt)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def print_sample(target_status: str, limit: int = 10):\n",
    "    printed = 0\n",
    "    with open(classified_path, newline=\"\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            status = (row.get(\"status\") or \"\").strip().replace(\" \", \"\")\n",
    "            if status != target_status:\n",
    "                continue\n",
    "            res_date = (row.get(\"sms_resDate\") or \"\").strip()\n",
    "            dt = parse_dt(res_date)\n",
    "            if not dt or dt.year != 2025:\n",
    "                continue\n",
    "            print(row.get(\"sms_msg\"))\n",
    "            printed += 1\n",
    "            if printed >= limit:\n",
    "                break\n",
    "    print(\"printed\", printed)\n",
    "\n",
    "print(\"입항예정 샘플\")\n",
    "print_sample(\"입항예정\", 10)\n",
    "\n",
    "print(\"\")\n",
    "print(\"미정 샘플\")\n",
    "print_sample(\"미정\", 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 울릉경찰서 교통계(교통사고)_2022년도.csv\n",
      "Unique addresses to geocode: 82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Geocoding: 100%|██████████| 82/82 [02:43<00:00,  1.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: 울릉경찰서 교통계(교통사고)_2022년도_with_coords.csv\n",
      "Processing: 울릉경찰서 교통계(교통사고)_2023년도.csv\n",
      "Unique addresses to geocode: 93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Geocoding: 100%|██████████| 93/93 [03:04<00:00,  1.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: 울릉경찰서 교통계(교통사고)_2023년도_with_coords.csv\n",
      "Processing: 울릉경찰서 교통계(교통사고)_2025년도.csv\n",
      "Unique addresses to geocode: 82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Geocoding: 100%|██████████| 82/82 [02:33<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: 울릉경찰서 교통계(교통사고)_2025년도_with_coords.csv\n",
      "Processing: 울릉경찰서 교통계(교통사고)_2024년도.csv\n",
      "Unique addresses to geocode: 95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Geocoding: 100%|██████████| 95/95 [02:59<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: 울릉경찰서 교통계(교통사고)_2024년도_with_coords.csv\n",
      "Processing: 울릉경찰서 교통계(교통사고)_2021년도.csv\n",
      "Unique addresses to geocode: 91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Geocoding: 100%|██████████| 91/91 [04:40<00:00,  3.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: 울릉경찰서 교통계(교통사고)_2021년도_with_coords.csv\n",
      "Processing: 울릉경찰서 교통계(교통사고)_2020년도.csv\n",
      "Unique addresses to geocode: 78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Geocoding: 100%|██████████| 78/78 [02:25<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: 울릉경찰서 교통계(교통사고)_2020년도_with_coords.csv\n",
      "Processing: 울릉경찰서 교통계(교통사고)_2019년도.csv\n",
      "Unique addresses to geocode: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Geocoding: 100%|██████████| 64/64 [01:59<00:00,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: 울릉경찰서 교통계(교통사고)_2019년도_with_coords.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import unicodedata\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# 설정\n",
    "TOKEN_TRAFFIC_STATS = \"교통계\"\n",
    "TOKEN_ACCIDENT = \"교통사고\"\n",
    "TOKEN_YEAR = \"년도\"\n",
    "ADDRESS_COL = \"사고장소\"\n",
    "LAT_COL = \"latitude\"\n",
    "LON_COL = \"longitude\"\n",
    "\n",
    "def _normalize_name(name: str) -> str:\n",
    "    return unicodedata.normalize(\"NFC\", name)\n",
    "\n",
    "def _clean_address(addr):\n",
    "    \"\"\"\n",
    "    주소 문자열을 정제합니다. \n",
    "    예: '경북 울릉군 ...로 550 경북 울릉군 ...로 550 식당앞' -> '경상북도 울릉군 울릉순환로 550'\n",
    "    \"\"\"\n",
    "    if not isinstance(addr, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # 1. 도로명 주소 추출 (시/도 + 시/군/구 + 로/길 + 번호)\n",
    "    # 정규식: (경북|경상북도) (울릉군|...) (무슨로|무슨길) (숫자)\n",
    "    pattern_road = r'(([가-힣]+도|[가-힣]+시)\\s+[가-힣]+(시|군|구)\\s+[가-힣0-9]+(로|길)\\s+\\d+(-\\d+)?)'\n",
    "    match = re.search(pattern_road, addr)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    \n",
    "    # 2. 지번 주소 추출 (시/도 + 시/군/구 + 읍/면/동 + 번호)\n",
    "    pattern_jibun = r'(([가-힣]+도|[가-힣]+시)\\s+[가-힣]+(시|군|구)\\s+[가-힣]+(읍|면|동)\\s+[가-힣0-9]+\\s+\\d+(-\\d+)?)'\n",
    "    match = re.search(pattern_jibun, addr)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "        \n",
    "    # 3. 추출 실패 시, 중복이라도 제거 시도 (공백 기준 split 후 중복 제거)\n",
    "    tokens = addr.split()\n",
    "    seen = set()\n",
    "    result = []\n",
    "    for token in tokens:\n",
    "        if token not in seen:\n",
    "            seen.add(token)\n",
    "            result.append(token)\n",
    "    \n",
    "    return \" \".join(result)\n",
    "\n",
    "def geocode_nominatim(address: str) -> tuple[float | None, float | None]:\n",
    "    \"\"\"Nominatim(OSM)을 사용하여 무료로 좌표를 조회합니다.\"\"\"\n",
    "    url = \"https://nominatim.openstreetmap.org/search\"\n",
    "    params = {\"q\": address, \"format\": \"json\", \"limit\": 1}\n",
    "    # Nominatim은 User-Agent가 필수입니다.\n",
    "    headers = {\"User-Agent\": \"traffic-analysis-script/1.0\"}\n",
    "    \n",
    "    try:\n",
    "        resp = requests.get(url, params=params, headers=headers, timeout=10)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        if not data:\n",
    "            return None, None\n",
    "        return float(data[0][\"lat\"]), float(data[0][\"lon\"])\n",
    "    except Exception as e:\n",
    "        print(f\"Error geocoding {address}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def process_file(path: Path):\n",
    "    print(f\"Processing: {path.name}\")\n",
    "    \n",
    "    # 여러 인코딩 시도\n",
    "    df = None\n",
    "    for enc in (\"utf-8-sig\", \"utf-8\", \"cp949\", \"euc-kr\"):\n",
    "        try:\n",
    "            df = pd.read_csv(path, encoding=enc)\n",
    "            break\n",
    "        except Exception:\n",
    "            continue\n",
    "            \n",
    "    if df is None:\n",
    "        print(f\"Failed to read {path.name}\")\n",
    "        return\n",
    "\n",
    "    # 컬럼명 공백 제거\n",
    "    df.columns = [str(c).strip() for c in df.columns]\n",
    "    \n",
    "    if ADDRESS_COL not in df.columns:\n",
    "        print(f\"Skipping: '{ADDRESS_COL}' column not found.\")\n",
    "        return\n",
    "\n",
    "    # 주소 정제\n",
    "    df[\"_clean_addr\"] = df[ADDRESS_COL].apply(_clean_address)\n",
    "    unique_addrs = df[\"_clean_addr\"].unique()\n",
    "    print(f\"Unique addresses to geocode: {len(unique_addrs)}\")\n",
    "\n",
    "    # 좌표 캐시 (메모리)\n",
    "    addr_cache = {}\n",
    "\n",
    "    from tqdm import tqdm # 진행률 표시 (없으면 제거 가능)\n",
    "    \n",
    "    # 지오코딩 수행\n",
    "    for addr in tqdm(unique_addrs, desc=\"Geocoding\"):\n",
    "        if not addr:\n",
    "            continue\n",
    "        if addr in addr_cache:\n",
    "            continue\n",
    "            \n",
    "        lat, lon = geocode_nominatim(addr)\n",
    "        if lat and lon:\n",
    "            addr_cache[addr] = (lat, lon)\n",
    "        else:\n",
    "            # 실패 시 원본 주소로 한 번 더 시도해볼 수도 있음\n",
    "            pass\n",
    "            \n",
    "        # Nominatim 정책 준수 (초당 1회 제한)\n",
    "        time.sleep(1.0)\n",
    "\n",
    "    # 데이터프레임에 매핑\n",
    "    df[LAT_COL] = df[\"_clean_addr\"].map(lambda x: addr_cache.get(x, (None, None))[0])\n",
    "    df[LON_COL] = df[\"_clean_addr\"].map(lambda x: addr_cache.get(x, (None, None))[1])\n",
    "    \n",
    "    # 불필요한 임시 컬럼 삭제\n",
    "    df = df.drop(columns=[\"_clean_addr\"])\n",
    "\n",
    "    # 저장\n",
    "    out_path = path.with_name(f\"{path.stem}_with_coords.csv\")\n",
    "    df.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"Saved to: {out_path}\")\n",
    "\n",
    "def main():\n",
    "    # 현재 디렉토리에서 대상 파일 찾기\n",
    "    root = Path(\".\")\n",
    "    targets = []\n",
    "    for f in root.iterdir():\n",
    "        if not f.is_file(): continue\n",
    "        name = _normalize_name(f.name)\n",
    "        if not name.endswith(\".csv\"): continue\n",
    "        # 파일명 조건 확인\n",
    "        if TOKEN_TRAFFIC_STATS in name and TOKEN_ACCIDENT in name:\n",
    "            targets.append(f)\n",
    "            \n",
    "    if not targets:\n",
    "        print(\"No target CSV files found.\")\n",
    "        # 만약 파일을 못 찾으면 모든 csv를 대상으로 하려면 아래 주석 해제\n",
    "        # targets = list(root.glob(\"*.csv\"))\n",
    "    \n",
    "    for target in targets:\n",
    "        process_file(target)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
